{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a cleaning / preporcessing pipeline as a function\n",
    "- split the data\n",
    "- do eda\n",
    "- find profanity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "#import advertools as adv\n",
    "import demoji\n",
    "import json\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "import warnings\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word = stopwords.words('english')\n",
    "warnings.filterwarnings(action=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/cyberbullying_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47692, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning steps\n",
    "\n",
    "- do eda on all the datzset \n",
    "- replace emojis with word\n",
    "- keep hastags, but remove # \n",
    "- remove usernames \n",
    "- remove punctuation\n",
    "- remove stop words and two letter words\n",
    "- tokenize\n",
    "\n",
    "\n",
    "- find profanity words using word2vec or not <>\n",
    "- add more words to the list of profanity???\n",
    "- remove non english words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_clean(df):\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open('../data/external/profanity_list.json') \n",
    "    profanity_data = json.load(f)\n",
    "\n",
    "    # extract emojis\n",
    "    def extract_emoji(txt):\n",
    "        emoji_txt = demoji.findall(txt)\n",
    "        emoji_keys = emoji_txt.keys()\n",
    "        emoji_values = emoji_txt.values()\n",
    "        return  ' '.join(list(map(str, emoji_keys))), ' '.join(list(map(str, emoji_values)))\n",
    "\n",
    "    # extract hashtags\n",
    "    def hashtags(txt):\n",
    "        txt = re.findall(\"#([a-zA-Z0-9_]{1,50})\", txt)\n",
    "        return ' '.join(list(map(str, txt)))\n",
    "    \n",
    "    # extract mentions\n",
    "    def mentions(txt):\n",
    "        txt = re.findall(\"@([a-zA-Z0-9_]{1,50})\", txt)\n",
    "        return ' '.join(list(map(str, txt)))\n",
    "\n",
    "    # lookup profanity manually\n",
    "    def find_profanity(text):\n",
    "        profanity_set = set() \n",
    "        for word in text.split(): \n",
    "            if word in profanity_data:\n",
    "                profanity_set.add(word)\n",
    "        return ' '.join(list(map(str, profanity_set)))\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    df['profanity_list'] = df['tweet_text'].apply(lambda x: find_profanity(x))\n",
    "    df['hashtags'] = df['tweet_text'].apply(lambda x: hashtags(x))\n",
    "    df['mentions'] = df['tweet_text'].apply(lambda x: mentions(x))\n",
    "    df['emoji_face'] =  df['tweet_text'].apply(lambda x: extract_emoji(x)[0])\n",
    "    df['emoji_names'] =  df['tweet_text'].apply(lambda x:extract_emoji(x)[1])\n",
    "\n",
    "    # remove stopwords\n",
    "    df['clean_txt'] = df['tweet_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word or len(word) > 2))\n",
    "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",\"\", x))\n",
    "    #remove hashtags\n",
    "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",\"\", x))\n",
    "\n",
    "    df['clean_txt_emoji'] =   df['clean_txt'] + \" \" + df['emoji_names'] # add emoji as text to tweet\n",
    "    df['clean_txt_emoji'] =  df['clean_txt_emoji'].str.lower() # to lowe case\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word)) #remove stop words\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x)) # remove links\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.replace(\"[^a-zA-Z#]\", \" \") # puntuation\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 2))\n",
    "    df['tokenized'] = df['clean_txt_emoji'].apply(word_tokenize) # tokenize\n",
    "    df['pos_tags'] = df['tokenized'].apply(nltk.tag.pos_tag) # add pos tag\n",
    "    df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x]) # extract a new pos tag\n",
    "    df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [ lemmatizer.lemmatize(word, tag) for word, tag in x]) # lemmatize\n",
    "    df['clean_txt_emoji'] =   df['clean_txt'] + \" \" + df['emoji_names'] + \" \" + df['hashtags']\n",
    "   \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning_time < 5 minutes\n",
    "clean_df = super_clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After cleaning \n",
    "- Lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45947</th>\n",
       "      <td>“@OldEngliiish: Hi nigger ... fuck ya life &amp;am...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>nigger fuck</td>\n",
       "      <td>bars freethisdick</td>\n",
       "      <td>OldEngliiish</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>“: Hi nigger ... fuck ya life &amp;amp; ya Bestfri...</td>\n",
       "      <td>“: Hi nigger ... fuck ya life &amp;amp; ya Bestfri...</td>\n",
       "      <td>[nigger, fuck, life, amp, bestfriend, lmfao, d...</td>\n",
       "      <td>[(nigger, NN), (fuck, JJ), (life, NN), (amp, J...</td>\n",
       "      <td>[(nigger, n), (fuck, a), (life, n), (amp, a), ...</td>\n",
       "      <td>[nigger, fuck, life, amp, bestfriend, lmfao, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27614</th>\n",
       "      <td>@iglvzx configuration setting. mentioning on i...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>iglvzx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>configuration setting. mentioning its own aga...</td>\n",
       "      <td>configuration setting. mentioning its own aga...</td>\n",
       "      <td>[configuration, setting, mentioning, dev, poli...</td>\n",
       "      <td>[(configuration, NN), (setting, VBG), (mention...</td>\n",
       "      <td>[(configuration, n), (setting, v), (mentioning...</td>\n",
       "      <td>[configuration, set, mention, dev, policy, iir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27283</th>\n",
       "      <td>@JeankyAz https://t.co/151ilFynIp</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>JeankyAz</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/151ilFynIp</td>\n",
       "      <td>https://t.co/151ilFynIp</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>Doing The Score For The Middle School, Reminds...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Doing The Score For The Middle School, Reminds...</td>\n",
       "      <td>Doing The Score For The Middle School, Reminds...</td>\n",
       "      <td>[score, middle, school, reminds, old, baskekba...</td>\n",
       "      <td>[(score, NN), (middle, JJ), (school, NN), (rem...</td>\n",
       "      <td>[(score, n), (middle, a), (school, n), (remind...</td>\n",
       "      <td>[score, middle, school, reminds, old, baskekba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   cyberbullying_type  \\\n",
       "45947  “@OldEngliiish: Hi nigger ... fuck ya life &am...            ethnicity   \n",
       "27614  @iglvzx configuration setting. mentioning on i...  other_cyberbullying   \n",
       "27283                  @JeankyAz https://t.co/151ilFynIp  other_cyberbullying   \n",
       "3669   Doing The Score For The Middle School, Reminds...    not_cyberbullying   \n",
       "\n",
       "      profanity_list           hashtags      mentions emoji_face emoji_names  \\\n",
       "45947    nigger fuck  bars freethisdick  OldEngliiish                          \n",
       "27614                                          iglvzx                          \n",
       "27283                                        JeankyAz                          \n",
       "3669                                                                           \n",
       "\n",
       "                                               clean_txt  \\\n",
       "45947  “: Hi nigger ... fuck ya life &amp; ya Bestfri...   \n",
       "27614   configuration setting. mentioning its own aga...   \n",
       "27283                            https://t.co/151ilFynIp   \n",
       "3669   Doing The Score For The Middle School, Reminds...   \n",
       "\n",
       "                                         clean_txt_emoji  \\\n",
       "45947  “: Hi nigger ... fuck ya life &amp; ya Bestfri...   \n",
       "27614   configuration setting. mentioning its own aga...   \n",
       "27283                          https://t.co/151ilFynIp     \n",
       "3669   Doing The Score For The Middle School, Reminds...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "45947  [nigger, fuck, life, amp, bestfriend, lmfao, d...   \n",
       "27614  [configuration, setting, mentioning, dev, poli...   \n",
       "27283                                                 []   \n",
       "3669   [score, middle, school, reminds, old, baskekba...   \n",
       "\n",
       "                                                pos_tags  \\\n",
       "45947  [(nigger, NN), (fuck, JJ), (life, NN), (amp, J...   \n",
       "27614  [(configuration, NN), (setting, VBG), (mention...   \n",
       "27283                                                 []   \n",
       "3669   [(score, NN), (middle, JJ), (school, NN), (rem...   \n",
       "\n",
       "                                             wordnet_pos  \\\n",
       "45947  [(nigger, n), (fuck, a), (life, n), (amp, a), ...   \n",
       "27614  [(configuration, n), (setting, v), (mentioning...   \n",
       "27283                                                 []   \n",
       "3669   [(score, n), (middle, a), (school, n), (remind...   \n",
       "\n",
       "                                              lemmatized  \n",
       "45947  [nigger, fuck, life, amp, bestfriend, lmfao, d...  \n",
       "27614  [configuration, set, mention, dev, policy, iir...  \n",
       "27283                                                 []  \n",
       "3669   [score, middle, school, reminds, old, baskekba...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3051: has non english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('words')\n",
    "# words = set(nltk.corpus.words.words())\n",
    "\n",
    "# sent = \"I work in google asdasb asnlkasn\"\n",
    "# \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "#          if w.lower() in words or not w.isalpha())\n",
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text            I have to poop but I'm the only one at my regi...\n",
       "cyberbullying_type                                    not_cyberbullying\n",
       "profanity_list                                                         \n",
       "hashtags                                                            fml\n",
       "mentions                                                               \n",
       "emoji_face                                                             \n",
       "emoji_names                                                            \n",
       "clean_txt                    I have poop but I'm the only one register \n",
       "clean_txt_emoji         I have poop but I'm the only one register   fml\n",
       "tokenized                                         [poop, one, register]\n",
       "pos_tags                        [(poop, NN), (one, CD), (register, NN)]\n",
       "wordnet_pos                        [(poop, n), (one, n), (register, n)]\n",
       "lemmatized                                        [poop, one, register]\n",
       "Name: 2640, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.iloc[2640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>❤ 😘</td>\n",
       "      <td>red heart face blowing a kiss</td>\n",
       "      <td>Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...</td>\n",
       "      <td>Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...</td>\n",
       "      <td>[kids, love, mohamad, bin, zayed, city, red, h...</td>\n",
       "      <td>[(kids, NNS), (love, VBP), (mohamad, JJ), (bin...</td>\n",
       "      <td>[(kids, n), (love, v), (mohamad, a), (bin, n),...</td>\n",
       "      <td>[kid, love, mohamad, bin, zayed, city, red, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>@iMrBarfield it starts Thursday. My classes ar...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>iMrBarfield</td>\n",
       "      <td>😌</td>\n",
       "      <td>relieved face</td>\n",
       "      <td>starts Thursday. My classes are canceled for ...</td>\n",
       "      <td>starts Thursday. My classes are canceled for ...</td>\n",
       "      <td>[starts, thursday, classes, canceled, tomorrow...</td>\n",
       "      <td>[(starts, NNS), (thursday, NN), (classes, VBZ)...</td>\n",
       "      <td>[(starts, n), (thursday, n), (classes, v), (ca...</td>\n",
       "      <td>[start, thursday, class, cancel, tomorrow, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>mmmm #MKR Forget Deconstruction @lisamromano @...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "      <td>lisamromano garydlum mattsparks88</td>\n",
       "      <td>🍋</td>\n",
       "      <td>lemon</td>\n",
       "      <td>mmmm  Forget Deconstruction    THIS Lemon Tart...</td>\n",
       "      <td>mmmm  Forget Deconstruction    THIS Lemon Tart...</td>\n",
       "      <td>[mmmm, forget, deconstruction, lemon, tart, mm...</td>\n",
       "      <td>[(mmmm, NN), (forget, VB), (deconstruction, NN...</td>\n",
       "      <td>[(mmmm, n), (forget, v), (deconstruction, n), ...</td>\n",
       "      <td>[mmmm, forget, deconstruction, lemon, tart, mm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>RT @mykitchenrules: Nawwww 😭😭😭 #MKR</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "      <td>mykitchenrules</td>\n",
       "      <td>😭</td>\n",
       "      <td>loudly crying face</td>\n",
       "      <td>RT : Nawwww 😭😭😭</td>\n",
       "      <td>RT : Nawwww 😭😭😭  loudly crying face MKR</td>\n",
       "      <td>[nawwww, loudly, crying, face]</td>\n",
       "      <td>[(nawwww, RB), (loudly, RB), (crying, VBG), (f...</td>\n",
       "      <td>[(nawwww, r), (loudly, r), (crying, v), (face,...</td>\n",
       "      <td>[nawwww, loudly, cry, face]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>@justinbieber Your How I Get Through Bullying ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>justinbieber</td>\n",
       "      <td>♥</td>\n",
       "      <td>heart suit</td>\n",
       "      <td>Your How I Get Through Bullying And Tough Tim...</td>\n",
       "      <td>Your How I Get Through Bullying And Tough Tim...</td>\n",
       "      <td>[get, bullying, tough, times, thinking, got, m...</td>\n",
       "      <td>[(get, VB), (bullying, JJ), (tough, JJ), (time...</td>\n",
       "      <td>[(get, v), (bullying, a), (tough, a), (times, ...</td>\n",
       "      <td>[get, bullying, tough, time, think, get, make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42964</th>\n",
       "      <td>RT @syeoga: Hella is from the bay....and major...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>bitches</td>\n",
       "      <td></td>\n",
       "      <td>syeoga</td>\n",
       "      <td>©</td>\n",
       "      <td>copyright</td>\n",
       "      <td>RT : Hella from the bay....and majority LA bit...</td>\n",
       "      <td>RT : Hella from the bay....and majority LA bit...</td>\n",
       "      <td>[hella, bay, and, majority, bitches, hate, wor...</td>\n",
       "      <td>[(hella, NN), (bay, NN), (and, CC), (majority,...</td>\n",
       "      <td>[(hella, n), (bay, n), (and, n), (majority, n)...</td>\n",
       "      <td>[hella, bay, and, majority, bitch, hate, word,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46427</th>\n",
       "      <td>— @oibanai ije, ur one of my faves but u alr k...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>oibanai</td>\n",
       "      <td>☺</td>\n",
       "      <td>smiling face</td>\n",
       "      <td>—  ije, ur one faves but u alr know that hehe ...</td>\n",
       "      <td>—  ije, ur one faves but u alr know that hehe ...</td>\n",
       "      <td>[ije, one, faves, alr, know, hehe, also, love,...</td>\n",
       "      <td>[(ije, VB), (one, CD), (faves, VBZ), (alr, RB)...</td>\n",
       "      <td>[(ije, v), (one, n), (faves, v), (alr, r), (kn...</td>\n",
       "      <td>[ije, one, faves, alr, know, hehe, also, love,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46879</th>\n",
       "      <td>@ksorbs 😂😆😂 #classic #coon... As pawpaw would ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td>classic coon momentsimissthatoldmanofhonor hil...</td>\n",
       "      <td>ksorbs</td>\n",
       "      <td>🙊 😂 😆</td>\n",
       "      <td>speak-no-evil monkey face with tears of joy gr...</td>\n",
       "      <td>😂😆😂  ... As pawpaw would DO 🙊 YEP I went ther...</td>\n",
       "      <td>😂😆😂  ... As pawpaw would DO 🙊 YEP I went ther...</td>\n",
       "      <td>[pawpaw, would, yep, went, there, speak, evil,...</td>\n",
       "      <td>[(pawpaw, NN), (would, MD), (yep, VB), (went, ...</td>\n",
       "      <td>[(pawpaw, n), (would, n), (yep, v), (went, v),...</td>\n",
       "      <td>[pawpaw, would, yep, go, there, speak, evil, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47610</th>\n",
       "      <td>@Lilcruz2 you really tryna get me in trouble o...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td>coon</td>\n",
       "      <td>Lilcruz2</td>\n",
       "      <td>😂</td>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>you really tryna get trouble twitter😂😂😂</td>\n",
       "      <td>you really tryna get trouble twitter😂😂😂  face...</td>\n",
       "      <td>[really, tryna, get, trouble, twitter, face, t...</td>\n",
       "      <td>[(really, RB), (tryna, JJ), (get, NN), (troubl...</td>\n",
       "      <td>[(really, r), (tryna, a), (get, n), (trouble, ...</td>\n",
       "      <td>[really, tryna, get, trouble, twitter, face, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47675</th>\n",
       "      <td>Y'all make sure y'all following a nigga #coon⛽...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>nigga</td>\n",
       "      <td>coon joeworld</td>\n",
       "      <td></td>\n",
       "      <td>🆖 💯 👌 🅰 😹 ⛽</td>\n",
       "      <td>NG button hundred points OK hand A button (blo...</td>\n",
       "      <td>Y'all make sure y'all following nigga ⛽🅰🆖💯lilb...</td>\n",
       "      <td>Y'all make sure y'all following nigga ⛽🅰🆖💯lilb...</td>\n",
       "      <td>[all, make, sure, all, following, nigga, lilbi...</td>\n",
       "      <td>[(all, DT), (make, VBP), (sure, JJ), (all, DT)...</td>\n",
       "      <td>[(all, n), (make, v), (sure, a), (all, n), (fo...</td>\n",
       "      <td>[all, make, sure, all, following, nigga, lilbi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "21     Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...  not_cyberbullying   \n",
       "80     @iMrBarfield it starts Thursday. My classes ar...  not_cyberbullying   \n",
       "135    mmmm #MKR Forget Deconstruction @lisamromano @...  not_cyberbullying   \n",
       "144                  RT @mykitchenrules: Nawwww 😭😭😭 #MKR  not_cyberbullying   \n",
       "151    @justinbieber Your How I Get Through Bullying ...  not_cyberbullying   \n",
       "...                                                  ...                ...   \n",
       "42964  RT @syeoga: Hella is from the bay....and major...          ethnicity   \n",
       "46427  — @oibanai ije, ur one of my faves but u alr k...          ethnicity   \n",
       "46879  @ksorbs 😂😆😂 #classic #coon... As pawpaw would ...          ethnicity   \n",
       "47610  @Lilcruz2 you really tryna get me in trouble o...          ethnicity   \n",
       "47675  Y'all make sure y'all following a nigga #coon⛽...          ethnicity   \n",
       "\n",
       "      profanity_list                                           hashtags  \\\n",
       "21                                                                        \n",
       "80                                                                        \n",
       "135                                                                 MKR   \n",
       "144                                                                 MKR   \n",
       "151                                                                       \n",
       "...              ...                                                ...   \n",
       "42964        bitches                                                      \n",
       "46427                                                                     \n",
       "46879                 classic coon momentsimissthatoldmanofhonor hil...   \n",
       "47610                                                              coon   \n",
       "47675          nigga                                      coon joeworld   \n",
       "\n",
       "                                mentions   emoji_face  \\\n",
       "21                                                ❤ 😘   \n",
       "80                           iMrBarfield            😌   \n",
       "135    lisamromano garydlum mattsparks88            🍋   \n",
       "144                       mykitchenrules            😭   \n",
       "151                         justinbieber            ♥   \n",
       "...                                  ...          ...   \n",
       "42964                             syeoga            ©   \n",
       "46427                            oibanai            ☺   \n",
       "46879                             ksorbs        🙊 😂 😆   \n",
       "47610                           Lilcruz2            😂   \n",
       "47675                                     🆖 💯 👌 🅰 😹 ⛽   \n",
       "\n",
       "                                             emoji_names  \\\n",
       "21                         red heart face blowing a kiss   \n",
       "80                                         relieved face   \n",
       "135                                                lemon   \n",
       "144                                   loudly crying face   \n",
       "151                                           heart suit   \n",
       "...                                                  ...   \n",
       "42964                                          copyright   \n",
       "46427                                       smiling face   \n",
       "46879  speak-no-evil monkey face with tears of joy gr...   \n",
       "47610                             face with tears of joy   \n",
       "47675  NG button hundred points OK hand A button (blo...   \n",
       "\n",
       "                                               clean_txt  \\\n",
       "21     Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...   \n",
       "80      starts Thursday. My classes are canceled for ...   \n",
       "135    mmmm  Forget Deconstruction    THIS Lemon Tart...   \n",
       "144                                     RT : Nawwww 😭😭😭    \n",
       "151     Your How I Get Through Bullying And Tough Tim...   \n",
       "...                                                  ...   \n",
       "42964  RT : Hella from the bay....and majority LA bit...   \n",
       "46427  —  ije, ur one faves but u alr know that hehe ...   \n",
       "46879   😂😆😂  ... As pawpaw would DO 🙊 YEP I went ther...   \n",
       "47610           you really tryna get trouble twitter😂😂😂    \n",
       "47675  Y'all make sure y'all following nigga ⛽🅰🆖💯lilb...   \n",
       "\n",
       "                                         clean_txt_emoji  \\\n",
       "21     Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...   \n",
       "80      starts Thursday. My classes are canceled for ...   \n",
       "135    mmmm  Forget Deconstruction    THIS Lemon Tart...   \n",
       "144              RT : Nawwww 😭😭😭  loudly crying face MKR   \n",
       "151     Your How I Get Through Bullying And Tough Tim...   \n",
       "...                                                  ...   \n",
       "42964  RT : Hella from the bay....and majority LA bit...   \n",
       "46427  —  ije, ur one faves but u alr know that hehe ...   \n",
       "46879   😂😆😂  ... As pawpaw would DO 🙊 YEP I went ther...   \n",
       "47610   you really tryna get trouble twitter😂😂😂  face...   \n",
       "47675  Y'all make sure y'all following nigga ⛽🅰🆖💯lilb...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "21     [kids, love, mohamad, bin, zayed, city, red, h...   \n",
       "80     [starts, thursday, classes, canceled, tomorrow...   \n",
       "135    [mmmm, forget, deconstruction, lemon, tart, mm...   \n",
       "144                       [nawwww, loudly, crying, face]   \n",
       "151    [get, bullying, tough, times, thinking, got, m...   \n",
       "...                                                  ...   \n",
       "42964  [hella, bay, and, majority, bitches, hate, wor...   \n",
       "46427  [ije, one, faves, alr, know, hehe, also, love,...   \n",
       "46879  [pawpaw, would, yep, went, there, speak, evil,...   \n",
       "47610  [really, tryna, get, trouble, twitter, face, t...   \n",
       "47675  [all, make, sure, all, following, nigga, lilbi...   \n",
       "\n",
       "                                                pos_tags  \\\n",
       "21     [(kids, NNS), (love, VBP), (mohamad, JJ), (bin...   \n",
       "80     [(starts, NNS), (thursday, NN), (classes, VBZ)...   \n",
       "135    [(mmmm, NN), (forget, VB), (deconstruction, NN...   \n",
       "144    [(nawwww, RB), (loudly, RB), (crying, VBG), (f...   \n",
       "151    [(get, VB), (bullying, JJ), (tough, JJ), (time...   \n",
       "...                                                  ...   \n",
       "42964  [(hella, NN), (bay, NN), (and, CC), (majority,...   \n",
       "46427  [(ije, VB), (one, CD), (faves, VBZ), (alr, RB)...   \n",
       "46879  [(pawpaw, NN), (would, MD), (yep, VB), (went, ...   \n",
       "47610  [(really, RB), (tryna, JJ), (get, NN), (troubl...   \n",
       "47675  [(all, DT), (make, VBP), (sure, JJ), (all, DT)...   \n",
       "\n",
       "                                             wordnet_pos  \\\n",
       "21     [(kids, n), (love, v), (mohamad, a), (bin, n),...   \n",
       "80     [(starts, n), (thursday, n), (classes, v), (ca...   \n",
       "135    [(mmmm, n), (forget, v), (deconstruction, n), ...   \n",
       "144    [(nawwww, r), (loudly, r), (crying, v), (face,...   \n",
       "151    [(get, v), (bullying, a), (tough, a), (times, ...   \n",
       "...                                                  ...   \n",
       "42964  [(hella, n), (bay, n), (and, n), (majority, n)...   \n",
       "46427  [(ije, v), (one, n), (faves, v), (alr, r), (kn...   \n",
       "46879  [(pawpaw, n), (would, n), (yep, v), (went, v),...   \n",
       "47610  [(really, r), (tryna, a), (get, n), (trouble, ...   \n",
       "47675  [(all, n), (make, v), (sure, a), (all, n), (fo...   \n",
       "\n",
       "                                              lemmatized  \n",
       "21     [kid, love, mohamad, bin, zayed, city, red, he...  \n",
       "80     [start, thursday, class, cancel, tomorrow, tho...  \n",
       "135    [mmmm, forget, deconstruction, lemon, tart, mm...  \n",
       "144                          [nawwww, loudly, cry, face]  \n",
       "151    [get, bullying, tough, time, think, get, make,...  \n",
       "...                                                  ...  \n",
       "42964  [hella, bay, and, majority, bitch, hate, word,...  \n",
       "46427  [ije, one, faves, alr, know, hehe, also, love,...  \n",
       "46879  [pawpaw, would, yep, go, there, speak, evil, m...  \n",
       "47610  [really, tryna, get, trouble, twitter, face, t...  \n",
       "47675  [all, make, sure, all, following, nigga, lilbi...  \n",
       "\n",
       "[440 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df[\"emoji_face\"].str.len() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source for lemmatixation and tokenization : https://www.holisticseo.digital/python-seo/nltk/lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list =  [lemmatizer.lemmatize(token, \"v\") for token in word_tokenize(words)]\n",
    "    sentence = \" \".join(lemma_list)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/processed/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freak work please I have many things to do doesnt\n"
     ]
    }
   ],
   "source": [
    "ss = \"freaking work please I have many things to does doesnt\"\n",
    "#clean_df['clean_txt_lemma'] = clean_df['super_clean_txt'].apply(lambda x: lemma(x))\n",
    "print(lemma(ss))\n",
    "#clean_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whole me being republican thing must reall'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"whole me being republican thing must reall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text            Dr..ur genuine and cleaver journelist , first ...\n",
       "cyberbullying_type                                            ethnicity\n",
       "profanity_list                                                         \n",
       "hashtags                                                               \n",
       "mentions                                                               \n",
       "emoji_face                                                             \n",
       "emoji_names                                                            \n",
       "clean_txt             Dr..ur genuine and cleaver journelist , first ...\n",
       "clean_txt_emoji       Dr..ur genuine and cleaver journelist , first ...\n",
       "tokenized             [genuine, cleaver, journelist, first, time, on...\n",
       "pos_tags              [(genuine, JJ), (cleaver, NN), (journelist, NN...\n",
       "wordnet_pos           [(genuine, a), (cleaver, n), (journelist, n), ...\n",
       "lemmatized            [genuine, cleaver, journelist, first, time, on...\n",
       "Name: 40953, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#28775 - bad cleaning\n",
    "#40953 - \"\"\n",
    "#24091 - \"\"\n",
    "# 46427 - abbrevaiton check ALR\n",
    "clean_df.iloc[40953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>katandandre mkr</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>In other words , your food was crapilicious!</td>\n",
       "      <td>In other words , your food was crapilicious!  ...</td>\n",
       "      <td>[words, food, crapilicious]</td>\n",
       "      <td>[(words, NNS), (food, NN), (crapilicious, JJ)]</td>\n",
       "      <td>[(words, n), (food, n), (crapilicious, a)]</td>\n",
       "      <td>[word, food, crapilicious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>aussietv MKR theblock ImACelebrityAU today sun...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Why  white?</td>\n",
       "      <td>Why  white?           aussietv MKR theblock Im...</td>\n",
       "      <td>[white]</td>\n",
       "      <td>[(white, JJ)]</td>\n",
       "      <td>[(white, a)]</td>\n",
       "      <td>[white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XochitlSuckkks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>classy whore? Or more red velvet cupcakes?</td>\n",
       "      <td>classy whore? Or more red velvet cupcakes?</td>\n",
       "      <td>[classy, whore, red, velvet, cupcakes]</td>\n",
       "      <td>[(classy, NN), (whore, NN), (red, JJ), (velvet...</td>\n",
       "      <td>[(classy, n), (whore, n), (red, a), (velvet, n...</td>\n",
       "      <td>[classy, whore, red, velvet, cupcake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Jason_Gio</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>meh. :P thanks for the heads up, but not too ...</td>\n",
       "      <td>meh. :P thanks for the heads up, but not too ...</td>\n",
       "      <td>[meh, thanks, heads, concerned, another, angry...</td>\n",
       "      <td>[(meh, JJ), (thanks, NNS), (heads, NNS), (conc...</td>\n",
       "      <td>[(meh, a), (thanks, n), (heads, n), (concerned...</td>\n",
       "      <td>[meh, thanks, head, concern, another, angry, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RudhoeEnglish</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This ISIS account pretending Kurdish account....</td>\n",
       "      <td>This ISIS account pretending Kurdish account....</td>\n",
       "      <td>[isis, account, pretending, kurdish, account, ...</td>\n",
       "      <td>[(isis, NN), (account, NN), (pretending, VBG),...</td>\n",
       "      <td>[(isis, n), (account, n), (pretending, v), (ku...</td>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "  profanity_list                                           hashtags  \\\n",
       "0                                                   katandandre mkr   \n",
       "1                 aussietv MKR theblock ImACelebrityAU today sun...   \n",
       "2                                                                     \n",
       "3                                                                     \n",
       "4                                                                     \n",
       "\n",
       "         mentions emoji_face emoji_names  \\\n",
       "0                                          \n",
       "1                                          \n",
       "2  XochitlSuckkks                          \n",
       "3       Jason_Gio                          \n",
       "4   RudhoeEnglish                          \n",
       "\n",
       "                                           clean_txt  \\\n",
       "0      In other words , your food was crapilicious!    \n",
       "1                               Why  white?            \n",
       "2         classy whore? Or more red velvet cupcakes?   \n",
       "3   meh. :P thanks for the heads up, but not too ...   \n",
       "4   This ISIS account pretending Kurdish account....   \n",
       "\n",
       "                                     clean_txt_emoji  \\\n",
       "0  In other words , your food was crapilicious!  ...   \n",
       "1  Why  white?           aussietv MKR theblock Im...   \n",
       "2       classy whore? Or more red velvet cupcakes?     \n",
       "3   meh. :P thanks for the heads up, but not too ...   \n",
       "4   This ISIS account pretending Kurdish account....   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                        [words, food, crapilicious]   \n",
       "1                                            [white]   \n",
       "2             [classy, whore, red, velvet, cupcakes]   \n",
       "3  [meh, thanks, heads, concerned, another, angry...   \n",
       "4  [isis, account, pretending, kurdish, account, ...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0     [(words, NNS), (food, NN), (crapilicious, JJ)]   \n",
       "1                                      [(white, JJ)]   \n",
       "2  [(classy, NN), (whore, NN), (red, JJ), (velvet...   \n",
       "3  [(meh, JJ), (thanks, NNS), (heads, NNS), (conc...   \n",
       "4  [(isis, NN), (account, NN), (pretending, VBG),...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0         [(words, n), (food, n), (crapilicious, a)]   \n",
       "1                                       [(white, a)]   \n",
       "2  [(classy, n), (whore, n), (red, a), (velvet, n...   \n",
       "3  [(meh, a), (thanks, n), (heads, n), (concerned...   \n",
       "4  [(isis, n), (account, n), (pretending, v), (ku...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                         [word, food, crapilicious]  \n",
       "1                                            [white]  \n",
       "2              [classy, whore, red, velvet, cupcake]  \n",
       "3  [meh, thanks, head, concern, another, angry, d...  \n",
       "4  [isi, account, pretend, kurdish, account, like...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Steemer or a Lemma\n",
    "##### to stem or to lemm? \n",
    "- Using both and comparing the result\n",
    "- knowing that lemma will go to the root word :; eg, lemma for better is good\n",
    "- Steemma for better is ? - beta?\n",
    "\n",
    "\n",
    "According to Stackoverflow: https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "Lemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.\n",
    "\n",
    " Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "\n",
    " The real difference between stemming and lemmatization is threefold:\n",
    "\n",
    "Stemming reduces word-forms to (pseudo)stems, whereas lemmatization reduces the word-forms to linguistically valid lemmas. This difference is apparent in languages with more complex morphology, but may be irrelevant for many IR applications;\n",
    "Lemmatization deals only with inflectional variance, whereas stemming may also deal with derivational variance;\n",
    "In terms of implementation, lemmatization is usually more sophisticated (especially for morphologically complex languages) and usually requires some sort of lexica. Satisfatory stemming, on the other hand, can be achieved with rather simple rule-based approaches.\n",
    "Lemmatization may also be backed up by a part-of-speech tagger in order to disambiguate homonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dsci572env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49c76ddf62ca98de74eedc2bb55b33e27315dcb91d5e2af2ea84b0e3308bb054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
