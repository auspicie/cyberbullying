{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thayeylolu/cyberbullying/blob/main/notebooks/cleaning_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNsQfVrY7kfe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GlP1qVwz7jyx"
      },
      "outputs": [],
      "source": [
        "### Imp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsX2yWyt7lw_"
      },
      "source": [
        "## Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sEWxlmuyJjKm"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "\n",
        "import demoji\n",
        "import json\n",
        "import pandas as pd\n",
        "import re, nltk\n",
        "import warnings\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_word = stopwords.words('english')\n",
        "warnings.filterwarnings(action=\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3-y37G87prR"
      },
      "source": [
        "## Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rgfeyPIXJjK4",
        "outputId": "0138ea79-dab5-4184-c02b-f31093db446c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"/content/drive/MyDrive/NLP/train_data.csv\"\n",
        "#url   = \"../data/raw/cyberbullying_tweets.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1gXiy-J7slI"
      },
      "source": [
        "## Checkin Unique Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NOmBjTrXJjK9",
        "outputId": "301d717d-2732-4be9-b850-dd9bd9f27a81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
              "       'age', 'ethnicity'], dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['cyberbullying_type'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHUNU96xJjLB"
      },
      "source": [
        "## data cleaning steps\n",
        "\n",
        "- do eda on all the datzset \n",
        "- replace emojis with word\n",
        "- keep hastags, but remove # \n",
        "- remove usernames \n",
        "- remove punctuation\n",
        "- remove stop words and two letter words\n",
        "- tokenize\n",
        "\n",
        "\n",
        "- find profanity words using word2vec or not <>\n",
        "- add more words to the list of profanity???\n",
        "- remove non english words\n",
        "- contraction \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyBw2BYQ7x4M"
      },
      "source": [
        "## List of possibel contractions in english\n",
        "- Sourced from  https://gist.github.com/MLWhiz/a603656c482ce13f3f2affc1d35f287d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dtWKNbSuJjLF"
      },
      "outputs": [],
      "source": [
        "# refernce : https://gist.github.com/MLWhiz/a603656c482ce13f3f2affc1d35f287d\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_uvS_s18UwY"
      },
      "source": [
        "## Replace contraction with  the full word\n",
        "\n",
        "Example: \n",
        "\n",
        "```python\n",
        "replace_contractions(\"this's a text with contraction\")\n",
        ">>> 'this is a text with contraction'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD4hpf0J8Tt5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "V-w2Ml0fJjLI",
        "outputId": "8572d0c7-bf9e-4fc4-c440-3e4414a932fe"
      },
      "outputs": [],
      "source": [
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrlZ_Leg9Dum"
      },
      "source": [
        "### Cleaning Function\n",
        "- Load Profanity Data sourced from  https://github.com/zacanger/profane-words/blob/master/words.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v6WnpZJEJjLM"
      },
      "outputs": [],
      "source": [
        "def super_clean(df):\n",
        "    profanity_url= \"/content/drive/MyDrive/NLP/profanity_list.json\"\n",
        "    #profanity_url= \"../data/external/profanity_list.json\"\n",
        "\n",
        "    # Opening JSON file\n",
        "    f = open(profanity_url) \n",
        "    profanity_data = json.load(f)\n",
        "\n",
        "    # extract emojis\n",
        "    def extract_emoji(txt):\n",
        "        emoji_txt = demoji.findall(txt)\n",
        "        emoji_keys = emoji_txt.keys()\n",
        "        emoji_values = emoji_txt.values()\n",
        "        return  ' '.join(list(map(str, emoji_keys))), ' '.join(list(map(str, emoji_values)))\n",
        "\n",
        "    # extract hashtags\n",
        "    def hashtags(txt):\n",
        "        txt = re.findall(\"#([a-zA-Z0-9_]{1,50})\", txt)\n",
        "        return ' '.join(list(map(str, txt)))\n",
        "    \n",
        "    # extract mentions\n",
        "    def mentions(txt):\n",
        "        txt = re.findall(\"@([a-zA-Z0-9_]{1,50})\", txt)\n",
        "        return ' '.join(list(map(str, txt)))\n",
        "\n",
        "    # lookup profanity manually\n",
        "    def find_profanity(text):\n",
        "        profanity_set = set() \n",
        "        for word in text.split(): \n",
        "            if word in profanity_data:\n",
        "                profanity_set.add(word)\n",
        "        return ' '.join(list(map(str, profanity_set)))\n",
        "\n",
        "      \n",
        "    df['profanity_list'] = df['tweet_text'].apply(lambda x: find_profanity(x)) # get curse words\n",
        "    df['hashtags'] = df['tweet_text'].apply(lambda x: hashtags(x)) # get hashtags\n",
        "    df['mentions'] = df['tweet_text'].apply(lambda x: mentions(x)) # get mentions\n",
        "    df['emoji_names'] =  df['tweet_text'].apply(lambda x:extract_emoji(x)[1]) # get names of emojis\n",
        "\n",
        "    \n",
        "    df['clean_txt'] = df['tweet_text'].apply(lambda x: replace_contractions(x)) # replace contraction\n",
        "    df['clean_txt'] = df['clean_txt'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word or len(word) > 2)) # remove stopwords\n",
        "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",\"\", x)) # mentions\n",
        "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",\"\", x)) # remove hashtags\n",
        "\n",
        "    df['clean_txt_emoji'] =   df['clean_txt'] + \" \" + df['emoji_names'] # add emoji as text to tweet\n",
        "    df['clean_txt_emoji'] =  df['clean_txt_emoji'].str.lower() # to lower case\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x)) # remove links\n",
        "    df['clean_txt_emoji'] =   df['clean_txt_emoji']+ \" \" + df['hashtags'] # add hashtags as text to tweet\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.replace(\"[^a-zA-Z#]\", \" \") # punctuation\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 2)) #remove stop words AGAIN\n",
        "   \n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.strip() # rmeove white space\n",
        "    df['tokenized'] = df['clean_txt_emoji'].apply(word_tokenize) # tokenize\n",
        "   \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAkH9Lo1-6Or"
      },
      "source": [
        "## Run cleaning Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WcuTFjaJJjLP"
      },
      "outputs": [],
      "source": [
        "# cleaning_time < 5 minutes\n",
        "clean_df = super_clean(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj28C01U_AVr"
      },
      "source": [
        "Save `clean_txt_emoji` and `cyberbullying_type` in a new dataframe called `train_df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jhOBjXUTJjLQ"
      },
      "outputs": [],
      "source": [
        "train_df = clean_df[['clean_txt_emoji', 'cyberbullying_type']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQbSEu4CJjLR"
      },
      "source": [
        "### After cleaning \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MVhCWk0vJjLT",
        "outputId": "af96758e-74d7-4c7b-9eb7-f9e981b149f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>profanity_list</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>mentions</th>\n",
              "      <th>emoji_names</th>\n",
              "      <th>clean_txt</th>\n",
              "      <th>clean_txt_emoji</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42667</th>\n",
              "      <td>99.2 is a shit radio station, those dumb fucks...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>shit fucks</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>99.2 shit radio station, those dumb fucks talk...</td>\n",
              "      <td>shit radio station those dumb fucks talk nigge...</td>\n",
              "      <td>[shit, radio, station, those, dumb, fucks, tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38285</th>\n",
              "      <td>Tell that to all the guys and girls who bullie...</td>\n",
              "      <td>age</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Tell that all the guys and girls who bullied m...</td>\n",
              "      <td>tell that all the guys and girls who bullied m...</td>\n",
              "      <td>[tell, that, all, the, guys, and, girls, who, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39139</th>\n",
              "      <td>Rebecca black was bullied too much and is now ...</td>\n",
              "      <td>age</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Rebecca black was bullied too much and now hom...</td>\n",
              "      <td>rebecca black was bullied too much and now hom...</td>\n",
              "      <td>[rebecca, black, was, bullied, too, much, and,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11121</th>\n",
              "      <td>Miley Cyrus makes date rape joke onstage at GA...</td>\n",
              "      <td>gender</td>\n",
              "      <td>rape</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Miley Cyrus makes date rape joke onstage GAY h...</td>\n",
              "      <td>miley cyrus makes date rape joke onstage gay</td>\n",
              "      <td>[miley, cyrus, makes, date, rape, joke, onstag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweet_text cyberbullying_type  \\\n",
              "42667  99.2 is a shit radio station, those dumb fucks...          ethnicity   \n",
              "38285  Tell that to all the guys and girls who bullie...                age   \n",
              "39139  Rebecca black was bullied too much and is now ...                age   \n",
              "11121  Miley Cyrus makes date rape joke onstage at GA...             gender   \n",
              "\n",
              "      profanity_list hashtags mentions emoji_names  \\\n",
              "42667     shit fucks                                 \n",
              "38285                                                \n",
              "39139                                                \n",
              "11121           rape                                 \n",
              "\n",
              "                                               clean_txt  \\\n",
              "42667  99.2 shit radio station, those dumb fucks talk...   \n",
              "38285  Tell that all the guys and girls who bullied m...   \n",
              "39139  Rebecca black was bullied too much and now hom...   \n",
              "11121  Miley Cyrus makes date rape joke onstage GAY h...   \n",
              "\n",
              "                                         clean_txt_emoji  \\\n",
              "42667  shit radio station those dumb fucks talk nigge...   \n",
              "38285  tell that all the guys and girls who bullied m...   \n",
              "39139  rebecca black was bullied too much and now hom...   \n",
              "11121       miley cyrus makes date rape joke onstage gay   \n",
              "\n",
              "                                               tokenized  \n",
              "42667  [shit, radio, station, those, dumb, fucks, tal...  \n",
              "38285  [tell, that, all, the, guys, and, girls, who, ...  \n",
              "39139  [rebecca, black, was, bullied, too, much, and,...  \n",
              "11121  [miley, cyrus, makes, date, rape, joke, onstag...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_df.sample(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L76YOkzt_dDU"
      },
      "source": [
        "### Save cleaned dataset and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "97NOo-srJjLb"
      },
      "outputs": [],
      "source": [
        "#clean_df_url = \"../data/processed/clean_df.csv\"\n",
        "clean_df_url = \"/content/drive/MyDrive/NLP/clean_df.csv\"\n",
        "clean_df.to_csv(clean_df_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nYMxwpwSJjLd"
      },
      "outputs": [],
      "source": [
        "#train_df_url= \"../data/processed/train_data.csv\"\n",
        "train_df_url =\"/content/drive/MyDrive/NLP/train_data.csv\"\n",
        "train_df.to_csv(train_df_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dfHUX9F-JjLg",
        "outputId": "001bfc37-d3e0-4728-fb1c-5c94c6fb4212"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_txt_emoji</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6228</th>\n",
              "      <td>woke this morning thinkin had school</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29766</th>\n",
              "      <td>given the changes recruiting not really surpri...</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19103</th>\n",
              "      <td>idiot doll part even don know what trying liby...</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15668</th>\n",
              "      <td>better try harder blondes</td>\n",
              "      <td>gender</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6187</th>\n",
              "      <td>love mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38452</th>\n",
              "      <td>yes years old sent this teenager who thinks cu...</td>\n",
              "      <td>age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17446</th>\n",
              "      <td>what else can say isalm tired this prophecy me...</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27721</th>\n",
              "      <td></td>\n",
              "      <td>other_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2612</th>\n",
              "      <td>survivor immunity idols have been issued mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12342</th>\n",
              "      <td>kat actually bitch but had laugh her response MKR</td>\n",
              "      <td>gender</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         clean_txt_emoji   cyberbullying_type\n",
              "6228                woke this morning thinkin had school    not_cyberbullying\n",
              "29766  given the changes recruiting not really surpri...  other_cyberbullying\n",
              "19103  idiot doll part even don know what trying liby...             religion\n",
              "15668                          better try harder blondes               gender\n",
              "6187                                            love mkr    not_cyberbullying\n",
              "38452  yes years old sent this teenager who thinks cu...                  age\n",
              "17446  what else can say isalm tired this prophecy me...             religion\n",
              "27721                                                     other_cyberbullying\n",
              "2612        survivor immunity idols have been issued mkr    not_cyberbullying\n",
              "12342  kat actually bitch but had laugh her response MKR               gender"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jW_Ljvo_YA2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FeauKBtDJjLj",
        "outputId": "43f46144-d54a-4990-d678-20e29a5cff21"
      },
      "outputs": [],
      "source": [
        "#28775 - bad cleaning\n",
        "#40953 - \"\"\n",
        "#24091 - \"\"\n",
        "# 46427 - abbrevaiton check ALR\n",
        "\n",
        "# 3051: has non english words\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('dsci572env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "49c76ddf62ca98de74eedc2bb55b33e27315dcb91d5e2af2ea84b0e3308bb054"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
