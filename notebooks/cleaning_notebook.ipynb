{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thayeylolu/cyberbullying/blob/main/notebooks/cleaning_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BNsQfVrY7kfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Imp"
      ],
      "metadata": {
        "id": "GlP1qVwz7jyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libaries"
      ],
      "metadata": {
        "id": "QsX2yWyt7lw_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEWxlmuyJjKm"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "\n",
        "import demoji\n",
        "import json\n",
        "import pandas as pd\n",
        "import re, nltk\n",
        "import warnings\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_word = stopwords.words('english')\n",
        "warnings.filterwarnings(action=\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read CSV"
      ],
      "metadata": {
        "id": "v3-y37G87prR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgfeyPIXJjK4",
        "outputId": "0138ea79-dab5-4184-c02b-f31093db446c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"/content/drive/MyDrive/NLP/train_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkin Unique Classes"
      ],
      "metadata": {
        "id": "p1gXiy-J7slI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOmBjTrXJjK9",
        "outputId": "301d717d-2732-4be9-b850-dd9bd9f27a81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
              "       'age', 'ethnicity'], dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['cyberbullying_type'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHUNU96xJjLB"
      },
      "source": [
        "## data cleaning steps\n",
        "\n",
        "- do eda on all the datzset \n",
        "- replace emojis with word\n",
        "- keep hastags, but remove # \n",
        "- remove usernames \n",
        "- remove punctuation\n",
        "- remove stop words and two letter words\n",
        "- tokenize\n",
        "\n",
        "\n",
        "- find profanity words using word2vec or not <>\n",
        "- add more words to the list of profanity???\n",
        "- remove non english words\n",
        "- contraction \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of possibel contractions in english\n",
        "- Sourced from  https://gist.github.com/MLWhiz/a603656c482ce13f3f2affc1d35f287d"
      ],
      "metadata": {
        "id": "FyBw2BYQ7x4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dtWKNbSuJjLF"
      },
      "outputs": [],
      "source": [
        "# refernce : https://gist.github.com/MLWhiz/a603656c482ce13f3f2affc1d35f287d\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace contraction with  the full word\n",
        "\n",
        "Example: \n",
        "\n",
        "```python\n",
        "replace_contractions(\"this's a text with contraction\")\n",
        ">>> 'this is a text with contraction'\n",
        "```\n"
      ],
      "metadata": {
        "id": "s_uvS_s18UwY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PD4hpf0J8Tt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V-w2Ml0fJjLI",
        "outputId": "8572d0c7-bf9e-4fc4-c440-3e4414a932fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fe648d6eebe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcontraction_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontraction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcontractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontractions_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplace_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fe648d6eebe4>\u001b[0m in \u001b[0;36m_get_contractions\u001b[0;34m(contraction_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcontraction_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontraction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontractions_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ],
      "source": [
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Function\n",
        "- Load Profanity Data sourced from  https://github.com/zacanger/profane-words/blob/master/words.json"
      ],
      "metadata": {
        "id": "YrlZ_Leg9Dum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6WnpZJEJjLM"
      },
      "outputs": [],
      "source": [
        "def super_clean(df):\n",
        "    profanity_ulr = \"/content/drive/MyDrive/NLP/profanity_list.json\"\n",
        "\n",
        "    # Opening JSON file\n",
        "    f = open(profanity_url) \n",
        "    profanity_data = json.load(f)\n",
        "\n",
        "    # extract emojis\n",
        "    def extract_emoji(txt):\n",
        "        emoji_txt = demoji.findall(txt)\n",
        "        emoji_keys = emoji_txt.keys()\n",
        "        emoji_values = emoji_txt.values()\n",
        "        return  ' '.join(list(map(str, emoji_keys))), ' '.join(list(map(str, emoji_values)))\n",
        "\n",
        "    # extract hashtags\n",
        "    def hashtags(txt):\n",
        "        txt = re.findall(\"#([a-zA-Z0-9_]{1,50})\", txt)\n",
        "        return ' '.join(list(map(str, txt)))\n",
        "    \n",
        "    # extract mentions\n",
        "    def mentions(txt):\n",
        "        txt = re.findall(\"@([a-zA-Z0-9_]{1,50})\", txt)\n",
        "        return ' '.join(list(map(str, txt)))\n",
        "\n",
        "    # lookup profanity manually\n",
        "    def find_profanity(text):\n",
        "        profanity_set = set() \n",
        "        for word in text.split(): \n",
        "            if word in profanity_data:\n",
        "                profanity_set.add(word)\n",
        "        return ' '.join(list(map(str, profanity_set)))\n",
        "\n",
        "      \n",
        "    df['profanity_list'] = df['tweet_text'].apply(lambda x: find_profanity(x)) # get curse words\n",
        "    df['hashtags'] = df['tweet_text'].apply(lambda x: hashtags(x)) # get hashtags\n",
        "    df['mentions'] = df['tweet_text'].apply(lambda x: mentions(x)) # get mentions\n",
        "    df['emoji_names'] =  df['tweet_text'].apply(lambda x:extract_emoji(x)[1]) # get names of emojis\n",
        "\n",
        "    \n",
        "    df['clean_txt'] = df['tweet_text'].apply(lambda x: replace_contractions(x)) # replace contraction\n",
        "    df['clean_txt'] = df['clean_txt'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word or len(word) > 2)) # remove stopwords\n",
        "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",\"\", x)) # mentions\n",
        "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",\"\", x)) # remove hashtags\n",
        "\n",
        "    df['clean_txt_emoji'] =   df['clean_txt'] + \" \" + df['emoji_names'] # add emoji as text to tweet\n",
        "    df['clean_txt_emoji'] =  df['clean_txt_emoji'].str.lower() # to lower case\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x)) # remove links\n",
        "    df['clean_txt_emoji'] =   df['clean_txt_emoji']+ \" \" + df['hashtags'] # add hashtags as text to tweet\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.replace(\"[^a-zA-Z#]\", \" \") # punctuation\n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 2)) #remove stop words AGAIN\n",
        "   \n",
        "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.strip() # rmeove white space\n",
        "    df['tokenized'] = df['clean_txt_emoji'].apply(word_tokenize) # tokenize\n",
        "   \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run cleaning Function"
      ],
      "metadata": {
        "id": "dAkH9Lo1-6Or"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcuTFjaJJjLP"
      },
      "outputs": [],
      "source": [
        "# cleaning_time < 5 minutes\n",
        "clean_df = super_clean(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save `clean_txt_emoji` and `cyberbullying_type` in a new dataframe called `train_df`"
      ],
      "metadata": {
        "id": "kj28C01U_AVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhOBjXUTJjLQ"
      },
      "outputs": [],
      "source": [
        "train_df = clean_df[['clean_txt_emoji', 'cyberbullying_type']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQbSEu4CJjLR"
      },
      "source": [
        "### After cleaning \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVhCWk0vJjLT",
        "outputId": "af96758e-74d7-4c7b-9eb7-f9e981b149f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>profanity_list</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>mentions</th>\n",
              "      <th>emoji_names</th>\n",
              "      <th>clean_txt</th>\n",
              "      <th>clean_txt_emoji</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30876</th>\n",
              "      <td>@TheQuinnspiracy i just got the jot script to ...</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TheQuinnspiracy</td>\n",
              "      <td></td>\n",
              "      <td>just got the jot script use with penultimate ...</td>\n",
              "      <td>got jot script use penultimate ipad</td>\n",
              "      <td>[got, jot, script, use, penultimate, ipad]</td>\n",
              "      <td>[(got, VBD), (jot, JJ), (script, NN), (use, NN...</td>\n",
              "      <td>[(got, v), (jot, a), (script, n), (use, n), (p...</td>\n",
              "      <td>[get, jot, script, use, penultimate, ipad]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>@tpw_rules nothings broken. I was just driving...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>tpw_rules</td>\n",
              "      <td></td>\n",
              "      <td>nothings broken. I was just driving througg l...</td>\n",
              "      <td>nothings broken driving througg lot water</td>\n",
              "      <td>[nothings, broken, driving, througg, lot, water]</td>\n",
              "      <td>[(nothings, NNS), (broken, VBN), (driving, VBG...</td>\n",
              "      <td>[(nothings, n), (broken, v), (driving, v), (th...</td>\n",
              "      <td>[nothing, break, drive, througg, lot, water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32771</th>\n",
              "      <td>I had a bully in elementary school I remember ...</td>\n",
              "      <td>age</td>\n",
              "      <td>fck</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>I had bully elementary school I remember was l...</td>\n",
              "      <td>bully elementary school remember lot bigger pu...</td>\n",
              "      <td>[bully, elementary, school, remember, lot, big...</td>\n",
              "      <td>[(bully, RB), (elementary, JJ), (school, NN), ...</td>\n",
              "      <td>[(bully, r), (elementary, a), (school, n), (re...</td>\n",
              "      <td>[bully, elementary, school, remember, lot, big...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19497</th>\n",
              "      <td>Disease is biggest terrorist !Its Jihad agains...</td>\n",
              "      <td>religion</td>\n",
              "      <td>terrorist terror</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Disease biggest terrorist !Its Jihad against H...</td>\n",
              "      <td>disease biggest terrorist its jihad humanity n...</td>\n",
              "      <td>[disease, biggest, terrorist, its, jihad, huma...</td>\n",
              "      <td>[(disease, NN), (biggest, JJS), (terrorist, NN...</td>\n",
              "      <td>[(disease, n), (biggest, a), (terrorist, n), (...</td>\n",
              "      <td>[disease, big, terrorist, it, jihad, humanity,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweet_text   cyberbullying_type  \\\n",
              "30876  @TheQuinnspiracy i just got the jot script to ...  other_cyberbullying   \n",
              "241    @tpw_rules nothings broken. I was just driving...    not_cyberbullying   \n",
              "32771  I had a bully in elementary school I remember ...                  age   \n",
              "19497  Disease is biggest terrorist !Its Jihad agains...             religion   \n",
              "\n",
              "         profanity_list hashtags         mentions emoji_names  \\\n",
              "30876                             TheQuinnspiracy               \n",
              "241                                     tpw_rules               \n",
              "32771               fck                                         \n",
              "19497  terrorist terror                                         \n",
              "\n",
              "                                               clean_txt  \\\n",
              "30876   just got the jot script use with penultimate ...   \n",
              "241     nothings broken. I was just driving througg l...   \n",
              "32771  I had bully elementary school I remember was l...   \n",
              "19497  Disease biggest terrorist !Its Jihad against H...   \n",
              "\n",
              "                                         clean_txt_emoji  \\\n",
              "30876               got jot script use penultimate ipad    \n",
              "241           nothings broken driving througg lot water    \n",
              "32771  bully elementary school remember lot bigger pu...   \n",
              "19497  disease biggest terrorist its jihad humanity n...   \n",
              "\n",
              "                                               tokenized  \\\n",
              "30876         [got, jot, script, use, penultimate, ipad]   \n",
              "241     [nothings, broken, driving, througg, lot, water]   \n",
              "32771  [bully, elementary, school, remember, lot, big...   \n",
              "19497  [disease, biggest, terrorist, its, jihad, huma...   \n",
              "\n",
              "                                                pos_tags  \\\n",
              "30876  [(got, VBD), (jot, JJ), (script, NN), (use, NN...   \n",
              "241    [(nothings, NNS), (broken, VBN), (driving, VBG...   \n",
              "32771  [(bully, RB), (elementary, JJ), (school, NN), ...   \n",
              "19497  [(disease, NN), (biggest, JJS), (terrorist, NN...   \n",
              "\n",
              "                                             wordnet_pos  \\\n",
              "30876  [(got, v), (jot, a), (script, n), (use, n), (p...   \n",
              "241    [(nothings, n), (broken, v), (driving, v), (th...   \n",
              "32771  [(bully, r), (elementary, a), (school, n), (re...   \n",
              "19497  [(disease, n), (biggest, a), (terrorist, n), (...   \n",
              "\n",
              "                                              lemmatized  \n",
              "30876         [get, jot, script, use, penultimate, ipad]  \n",
              "241         [nothing, break, drive, througg, lot, water]  \n",
              "32771  [bully, elementary, school, remember, lot, big...  \n",
              "19497  [disease, big, terrorist, it, jihad, humanity,...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_df.sample(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save cleaned dataset and train dataset"
      ],
      "metadata": {
        "id": "L76YOkzt_dDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97NOo-srJjLb"
      },
      "outputs": [],
      "source": [
        "\n",
        "clean_df.to_csv(\"../data/processed/clean_df.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYMxwpwSJjLd"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"../data/processed/train_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfHUX9F-JjLg",
        "outputId": "001bfc37-d3e0-4728-fb1c-5c94c6fb4212"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_txt_emoji</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45711</th>\n",
              "      <td>shut fuck dumb ass nigger since know full stor...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17218</th>\n",
              "      <td>neither dickriding arab people help all muslim...</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24228</th>\n",
              "      <td>would rather deal mra wadhwa</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29147</th>\n",
              "      <td>waiting bart home curl reflect away internet w...</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3946</th>\n",
              "      <td>nowhere yet secure funding hiring</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8201</th>\n",
              "      <td>females really getting annoyed man spams texts...</td>\n",
              "      <td>gender</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>love way</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38233</th>\n",
              "      <td>causing harm outing someone biological sex put...</td>\n",
              "      <td>age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1486</th>\n",
              "      <td>average MKR</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14065</th>\n",
              "      <td>dont know guys saw stupid media said miley mad...</td>\n",
              "      <td>gender</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         clean_txt_emoji   cyberbullying_type\n",
              "45711  shut fuck dumb ass nigger since know full stor...            ethnicity\n",
              "17218  neither dickriding arab people help all muslim...             religion\n",
              "24228                      would rather deal mra wadhwa   other_cyberbullying\n",
              "29147  waiting bart home curl reflect away internet w...  other_cyberbullying\n",
              "3946                  nowhere yet secure funding hiring     not_cyberbullying\n",
              "8201   females really getting annoyed man spams texts...               gender\n",
              "1035                                           love way     not_cyberbullying\n",
              "38233  causing harm outing someone biological sex put...                  age\n",
              "1486                                         average MKR    not_cyberbullying\n",
              "14065  dont know guys saw stupid media said miley mad...               gender"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_jW_Ljvo_YA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeauKBtDJjLj",
        "outputId": "43f46144-d54a-4990-d678-20e29a5cff21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tweet_text            Dr..ur genuine and cleaver journelist , first ...\n",
              "cyberbullying_type                                            ethnicity\n",
              "profanity_list                                                         \n",
              "hashtags                                                               \n",
              "mentions                                                               \n",
              "emoji_names                                                            \n",
              "clean_txt             Dr..ur genuine and cleaver journelist , first ...\n",
              "clean_txt_emoji       genuine cleaver journelist first time one spea...\n",
              "tokenized             [genuine, cleaver, journelist, first, time, on...\n",
              "pos_tags              [(genuine, JJ), (cleaver, NN), (journelist, NN...\n",
              "wordnet_pos           [(genuine, a), (cleaver, n), (journelist, n), ...\n",
              "lemmatized            [genuine, cleaver, journelist, first, time, on...\n",
              "Name: 40953, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#28775 - bad cleaning\n",
        "#40953 - \"\"\n",
        "#24091 - \"\"\n",
        "# 46427 - abbrevaiton check ALR\n",
        "\n",
        "# 3051: has non english words\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('dsci572env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "49c76ddf62ca98de74eedc2bb55b33e27315dcb91d5e2af2ea84b0e3308bb054"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}